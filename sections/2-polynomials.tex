

\section{Polynomials}
In this section we first give some basic definitions involving polynomials. Then we look at theorems that deal with the irreducibility of polynomials over $\Z$ and $\Q$ before briefly recalling the Fundamental Theorem of Algebra for polynomials over $\C$. 
\subsection{Basics of Polynomials}

\begin{definition}
    Let $R$ be an integral domain and $t$ be an indeterminate. A \textit{polynomial} $f$ over $R$ (in the indeterminate $t$) is of the form $f(t) = r_n t^n + r_{n-1} t^{n-1} + ... + r_1 t + r_0$, where the \textit{coefficients} $r_i \in R$. 
\end{definition}

We will be mainly working with polynomials over a field (recall that any field is an integral domain) but also polynomials over $\Z$. It is easy to check directly from the definitions above that the set of all polynomials over an integral $R$ obeys all the normal algebraic laws (including addition, multiplication, and division with remainder). We denoted the set by $R[t]$ and call it the ring of polynomials over $R$ (in the indeterminate $t$). 

\begin{definition}
	Let $R$ be an integral domain. Let $\alpha \in R$ and $f \in R[t]$. If $f(\alpha) = 0$, we say that $\alpha$ is a \textit{root} or a \textit{zero} of $f$. 
\end{definition}

\begin{theorem}[The Remainder Theorem] \label{thm:remainder}
	Let $f$ be a non-zero polynomial over an integral domain $R$ and let $\beta \in R$. Then $f \equiv f(\beta) \mod (t - \beta)$. 
	In particular, $\beta$ is a root of $f$ if and only if $(t - \beta) \mid f$. 
\end{theorem}


\begin{definition}
	Let $R$ be an integral domain. If $f \in R[t]$ and $f \neq 0$, then the \textit{degree} of $f$ is the highest power of $t$ occurring in $f$ with a non-zero coefficient, denoted as $\partial f$. If $f$ has degree $n$, then $a_n$ is the leading coefficient of $f$; if $a_n = 1$, we say that $f$ is monic. A polynomial with degree $0, 1, 2, 3, 4, 5$ is called constant, linear, quadratic, cubic, quartic, and quintic respectively.
%	When $f = 0$, we define $\partial f = - \infty$, where $-\infty < n, (-\infty) + (- \infty) = -\infty, -\infty + n = \infty$ for all $n \in \Z$. 
\end{definition}



%Polynomials with degree $n$ will often be written in descending order $r_n t^n + r_{n-1} t^{n+1} + ... + r_1 t + r_0$.

%\begin{theorem}
%    Two polynomials $f,g$ over $K$ define the same function if and only if they are equal polynomials, so they have the same coefficients.
%\end{theorem}
%
%\begin{proof}
%    This is a basic proof to an obvious theorem. By taking the difference of the two polynomials, we must prove that if $f(t)$ is a polynomial over $K$ and $f(t) = 0$ for all $t$, then the coefficients of $f$ are all 0. Let $P(n)$ be the statement: If a polynomial $f(t)$ over $K$ has degree $n$, and $f(t) = 0$ for all $t$, then $f=0$. We prove $P(n)$ for all $n$ by induction on $n$. 
%    
%    Both $P(0)$ and $P(1)$ are obvious. Suppose that $P(n-1)$ is true. With $f(t) = a_n t^n +...+ a_0$. In particular, $f(0) = 0$, so $a_0 = 0$ and,
%    $$
%    f(t) = a_n t^n +...+ a_1 t= t(a_n t^{n-1} +...+ a_1) = tg(t),
%    $$
%    where $g(t) = a_n t^{n-1} +...+ a_1$ has degree $n-1$. Now $g(t)$ vanishes for all $t$ except $t=0$. However is $g(0) = a_1 \neq 0$ then $g(t) \neq 0$ for sufficiently small $t$. Therefore $g(t)$ vanishes for all $t$. By induction, $g=0$. Therefore $f=0$ so $P(n)$ is true and the induction is complete.
%\end{proof}

%Two polynomials are defined to be equal if and only if all corresponding co-efficients are equal.

%If $r = \sum (r_i t^i)$ and $s = \sum (s_i t^i)$ then $r+s = \sum (r_i + s_i)t^i$, and $rs = \sum (q_j t^j)$ where $q_j = \sum_{h+i=j} r_h s_i$.


\begin{definition}
     A non-constant polynomial $f$ over an integral domain $R$ is \textit{reducible} if it is a product of two polynomials $g$ and $h$ over $R$ of smaller degree, in which case $g$ and $h$ are \textit{factors} of $f$. Otherwise $f$ is \textit{irreducible}. 
\end{definition}

\begin{theorem}
    Any non-zero polynomial over an integral domain $R$ is a product of irreducible polynomials over $R$.
\end{theorem}

\begin{proof}
    Let $g$ be any non-zero polynomial over $R$. We proceed by in introduction on the degree of $g$. If $\partial g = 0$ or 1 then $g$ is automatically irreducible. If $\partial g > 1$, then either $g$ is irreducible or $g = hk$ where $\partial h$, $\partial k < \partial g$. By induction, $h$ and $k$ are products of irreducible polynomials, where $g$ is such a product. The theorem then follows by induction.
\end{proof}

\begin{example}
    We can use the above theorem to prove irreducibilty for some cases, working very well for cubic polynomials over $\Z$. Consider polynomial $f(t) = t^3 -5 t + 1$ over $\Z$. If it was not irreducible then it must have a linear factor $t - \alpha$ over $\Z$ and then $\alpha \in \Z$ and $f(\alpha) = 0$. There also must exist $\beta, \lambda \in \Z$ such that,
    $$
    f(t) = (t-\alpha)(t^2 + \beta t + \lambda) = t^3 + (\beta - \alpha)t^2 + (\lambda - \alpha \beta)t - \alpha \lambda,
    $$
    so $\alpha \lambda = -1$. Therefore, $\alpha = \pm 1$. But $f(1) = -3 \neq 0$ and $f(-1) = 5 \neq 0$. This shows that no linear factor over $\Z$ of $f$ exists and hence $f$ is irreducible over $\Z$.
\end{example}




In an analogous way, we also define a polynomial $f$ over an integral domain $R$ in $n$ indeterminates $t_1, t_2, \dots, t_n$ as the form 
$$
f(t_1, \dots, t_n) = \sum _{ r_{\alpha_1, \dots, \alpha_n} \in R}  r_{\alpha_1, \dots, \alpha_n} t_1^{\alpha_1} \dots t_n ^{\alpha_n},
$$
where $\alpha_i$ are non-negative integers and only finitely many $r_{\alpha_1, \dots, \alpha_n}$ are non-zero. Such polynomials form the ring $R[t_1, \dots, t_n]$.

\begin{definition}
	Let $R$ be an integral domain and let $f(t_1, \dots, t_n) \in R[t_1, \dots, t_n]$. We say that $f$ is
    a \textit{symmetric polynomial} if $f$ remains the same when the indeterminates are interchanged. 
    Formally, for any $\sigma \in S_n$ (see Definition \ref{def:permutation}), we have $f(t_1,t_2,...,t_n) = f(t_{\sigma(1)},t_{\sigma(2)},...,t_{\sigma(n)})$.
\end{definition}

\begin{example}
    Consider the polynomial 
    $f(t_1,t_2)=t_1^2+t_2^2-4$, we can see that $f(t_2,t_1)=t_2^2+t_1^2-4$ and so $f(t_1,t_2)=f(t_2,t_1)$ and so $f(t_1,t_2)$ is a symmetric polynomial.
\end{example}

\begin{definition}
    The \textit{elementary symmetric polynomials} in $n$ indeterminates are defined by for $k = 1, \dots, n$,
    \begin{align*}
    e_k(t_1,t_2,...,t_n) = \sum_{1\leq j_1<j_2<...<j_k\leq n} t_{j_1}\cdot t_{j_2} \cdot ... \cdot t_{j_k}.
    \end{align*}
\end{definition}

\begin{example}
    For example, in the case where $n=4$, we can see the following are elementary symmetric polynomials: $e_1(t_1,t_2,t_3,t_4) = t_1 + t_2 + t_3 + t_4$, $e_2(t_1, t_2, t_3, t_4) = t_1t_2 + t_1t_3 + t_1t_4 + t_2t_3 + t_2t_4 + t_3t_4$,  $e_3(t_1,t_2,t_3,t_4) = t_1t_2t_3+t_1t_2t_4+t_1t_3t_4+t_2t_3t_4$, and $e_4(t_1, t_2, t_3, t_4) = t_1 t_2 t_3 t_4$. 
\end{example}


\subsection{Polynomials over $\Z$, $\Q$ and $\C$} 
We now give some theorems related to the irreducibility of polynomials over $\Z$ and $\Q$.

\begin{lemma}[Gauss' Lemma]
     Let $f$ be a polynomial over $\Z$ that is irreducible over $\Z$. Then $f$, considered as a polynomial over $\Q$, is also irreducible over $\Q$.
\end{lemma}

\begin{proof}
This lemma is useful because when we extend the subring of coefficients from $\Z$ to $\Q$ then there are new polynomials which may be factors of $f$. However, we now show that they are not. Starting with a contradiction, suppose that $f$ is irreducible over $\Z$ but reducible over $\Q$ so that $f = g h$ where $g$,$h$ are both polynomials over $\Q$, of a smaller degree. Multiplying through by the product of the denominators of the coefficients of $g$ and $h$ means we can rewrite this as $n f = g' h'$, where $n \in \Z$ and $g'$ and $h'$ are polynomials over $\Z$. We can now show that we can cancel out the prime factors of $n$ one by one, without going outside $\Z[t]$.

Suppose that $p$ is a prime factor of $n$. We now claim that if $g' = g_0 + g_1 t +...+ g_r t^r$, $h' = h_0 + h_1 t +...+ h_s t^s$ then either $p$ divides all the coefficients of $g_i$, or $p$ divides all the coefficients $h_j$.
If not then there must be smallest values $i$, $j$ such that $p \nmid g_i$ and $p \nmid h_j$. However, $p$ divides the coefficient of $t^{i+j}$ in $g' h'$, which is
$$
h_0 g_{i+j} + h_1 g_{i+j-1} +...+ h_j g_i +...+ h_{i+j} g_0
$$
and by the choice of $i$ and $j$, the prime $p$ divides every term of this expression except $h_j g_i$. $p$ divides the whole expression, so $p | h_j g_i$. However $p \nmid g_i$ and $p \nmid h_j$, a contradiction.
\end{proof}

\begin{theorem}[Eisenstein's Criterion] \label{thm:eisenstein}
     Let
    $f(t) = a_0 + a_1 t + ... + a_n t^n$
    be a polynomial over $\Z$. Suppose there is a prime $q$ such that
    (1) $q \nmid a_n$, 
    (2) $q \mid a_i$ for $i = {0, 1,..., n-1}$, and 
    (3) $q^2 \nmid a_0$. 
    Then $f$ is irreducible over $\Q$. 
\end{theorem}

\begin{proof}
By Gauss' Lemma it is sufficient to show that $f$ is irreducible over $\Z$. Suppose for a contradiction that $f = gh$, where
$
g(t)=b_0+b_1 t+ ... +b_r t^r,
$
and
$
h(t)=c_0+c_1 t+ ... +c_s t^s
$
are polynomials of smaller degree over $\Z$. Then $r \ge 1, s \ge 1$ and $r+s = n$. Now $b_0 c_0 = a_0$ so by (2), $q | b_0$ or $q|c_0$. By (3), $q$ cannot divide both $b_0$ and $c_0$, so without
loss of generality we can assume $q | b_0$ and $q \nmid c_0$. If all $b_j$ are divisible by $q$, then $a_n$ is divisible by $q$, contrary to (1). Let $b_j$ be the first coefficient of $g$ not divisible by $q$. Then
$
a_j = b_j c_0 + ...+ b_0 c_j
$
where $j < n$. This implies that $q$ divides $c_0$, since $q$ divides $a_j, b_0,..., b_{j-1}$, but not $b_j$. This is a contradiction. Hence $f$ is irreducible.
\end{proof}

\begin{example}
Consider$
f(t) = \frac{2}{9} t^5 + \frac{5}{3} t^4 + t^3 
$ over $\Q$. This is irreducible over $\Q$ if and only if $
9f(t) = 2t^5 + 15t^4 + 9t^3
$
is irreducible over $\Q$. Eisenstein's criterion now applies with $q = 3$, showing that $f$ is irreducible.
\end{example}

\begin{theorem}
If $p$ is prime, the binomial coefficient $\binom{p}{r}$ is divisible by $p$ if $1 \le r \le p-1$.
\end{theorem}

\begin{proof}
The binomial coefficient is an integer and $\binom{p}{r} = \frac{p!}{r!(p-r)!}$. The factor $p$ in the numerator cannot cancel with any factor in the denominator unless $r=0$ or $r=p$.
\end{proof}

\begin{theorem}\label{thm:irreducible-prime-polynomial}
    If $p$ is a prime then the polynomial
    $
    f(t) = 1 + t + ... + t^{p-1}
    $
    is irreducible over $\Q$.
\end{theorem}

\begin{proof}
First, $f(t) = \frac{t^p - 1}{t - 1}$. Put $t = 1 + u$ where $u$ is a new indeterminate. Then $f(t)$ is irreducible over $\Q$ if and only if $f(1+u)$ is irreducible. But
$$
f(1+u) = \frac{(1+u)^p - 1}{u}  = u^{p-1} + ph(u),
$$
where $h$ is a polynomial in $u$ over $\Z$ with constant term 1, by the above theorem. By Eisenstein's Criterion, $f(1+u)$ is irreducible over $\Q$.
\end{proof}

%After discovering that equations can be solved over $\C$, then there is a question of why stop at $\C$. Why not try and find an equation with no solutions over $\C$? The answer is that no such polynomial equations exist as 

The next theorem, taken from \cite{complex-functions-uon} without proof, states that every polynomial over $\C$ has a root over $\C$. We can further show that any polynomial over $\C$ can be written as a product of linear factors over $\C$. 


\begin{theorem}[Fundamental Theorem of Algebra] \label{thm:fundamental-algebra}
	If $p(z)$ is a non-constant polynomial over $\mathbb{C}$, then there exists $z_0 \in \mathbb{C}$ such that $p\left(z_0\right)=0$.
\end{theorem}

This is a special case where it only works over $\C$. Such a number $z$ is called a root of the equation $p(t)=0$. For example, $i$ is a root of the equation $t^2+1=0$ and a zero of $t^2+1$. Polynomial equations may have more than one root; indeed, $t^2+1=0$ has at least one other root, $-i$ but others will have more roots. We do the proof above by starting out with a contradiction.

\begin{proposition}
	Let $p(t) \in \mathbb{C}[t]$ with $\partial p=n \geq 1$. Then there exist $\alpha_1, \ldots, \alpha_n \in \mathbb{C}$, and $0 \neq k \in \mathbb{C}$, such that
	$
	p(t)=k\left(t-\alpha_1\right) \ldots\left(t-\alpha_n\right).
	$
\end{proposition}

\begin{proof}
	Use induction on $n$. When $n = 1$ then it is obvious that this proposition follows. If $n > 1$ then we know by Theorem \ref{thm:fundamental-algebra}, that $p(t)$ has at least one zero $\alpha_n$. By Theorem \ref{thm:remainder}, there exists $q(t) \in \C[t]$ such that,
		$p(t) = (t-\alpha_n) q(t)$. 
	Then $\partial q = n - 1$, so by induction,
		$q(t) = k(t-\alpha_a)...(t-\alpha_{n-1})$. 
	For suitable complex numbers $k,\alpha_1,...,\alpha_{n-1}$. Substitute $q(t)$ into $p(t)$ and the induction step is complete.
\end{proof}